{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dace9ed9-84ea-43e6-8203-9129d136f595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"kubeflownotebookswg/jupyter-pytorch-full:v1.8.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3de6bd2-789f-4ccf-bf15-ee01dc4a95de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import Input, Output\n",
    "from kfp.dsl import Dataset, Artifact\n",
    "from kfp.dsl import Model, Metrics, ClassificationMetrics\n",
    "\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "841d752a-a222-460e-a76c-86233f0a129d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def load_data(\n",
    "    x_train_pickle: Output[Dataset],\n",
    "    y_train_pickle: Output[Dataset],\n",
    "    x_test_pickle: Output[Dataset],\n",
    "    y_test_pickle: Output[Dataset],\n",
    "):\n",
    "    # import dataset\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    \n",
    "    # load dataset    \n",
    "    subset_indices = list(range(3000))\n",
    "    x_train, y_train = torch.utils.data.Subset(torchvision.datasets.MNIST('.', \n",
    "                             train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),subset_indices)\n",
    "    subset_indices = list(range(1000))\n",
    "    x_test, y_test = torch.utils.data.Subset(torchvision.datasets.MNIST('.', \n",
    "                                train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                                ])),subset_indices)\n",
    "    \n",
    "    # count the number of unique train labels\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"Train labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "    # count the number of unique test labels\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    print(\"\\nTest labels: \", dict(zip(unique, counts)))\n",
    "    indexes = np.random.randint(0, x_train.shape[0], size=25)\n",
    "    images = x_train[indexes]\n",
    "    with open(x_train_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(x_train, file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(y_train, file)\n",
    "\n",
    "    with open(x_test_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(x_test, file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ef6a86a-cb8d-41e5-83c6-7e89c05a8f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=BASE_IMAGE)\n",
    "def preprocess_data(\n",
    "    x_train_pickle: Input[Dataset],\n",
    "    y_train_pickle: Input[Dataset],\n",
    "    x_test_pickle: Input[Dataset],\n",
    "    y_test_pickle: Input[Dataset],\n",
    "    x_train_prep: Output[Dataset],\n",
    "    y_train_prep: Output[Dataset],\n",
    "    x_test_prep: Output[Dataset],\n",
    "    y_test_prep: Output[Dataset],\n",
    ") -> NamedTuple(\"outputs\", input_size=int, num_labels=int):\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from typing import NamedTuple\n",
    "\n",
    "    with open(x_train_pickle.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "\n",
    "    with open(x_test_pickle.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)\n",
    "\n",
    "    num_labels = len(np.unique(y_train))\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    image_size = x_train.shape[1]\n",
    "    input_size = image_size * image_size\n",
    "    # resize and normalize\n",
    "    x_train = np.reshape(x_train, [-1, input_size])\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = np.reshape(x_test, [-1, input_size])\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "    with open(x_train_prep.path, \"wb\") as file:\n",
    "        pickle.dump(x_train, file)\n",
    "\n",
    "    with open(y_train_prep.path, \"wb\") as file:\n",
    "        pickle.dump(y_train, file)\n",
    "\n",
    "    with open(x_test_prep.path, \"wb\") as file:\n",
    "        pickle.dump(x_test, file)\n",
    "\n",
    "    with open(y_test_prep.path, \"wb\") as file:\n",
    "        pickle.dump(y_test, file)\n",
    "    outputs = NamedTuple(\"outputs\", input_size=int, num_labels=int)\n",
    "    return outputs(input_size, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cd53b86-c71a-4308-93fc-08966d34b96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=BASE_IMAGE)\n",
    "def train(\n",
    "    input_size: int,\n",
    "    num_labels: int,\n",
    "    epochs: int,\n",
    "    x_train_pickle: Input[Dataset],\n",
    "    y_train_pickle: Input[Dataset],\n",
    "    model_artifact: Output[Model],\n",
    "    log: Output[Artifact],\n",
    "):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(320, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x)     \n",
    "    \n",
    "\n",
    "    batch_size_train = 100\n",
    "    batch_size_test = 100\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.5\n",
    "    log_interval = 10\n",
    "\n",
    "    random_seed = 1\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    with open(x_train_pickle.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "\n",
    "    log_dir = f\"{log.path}/logs/fit/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Running on {device}.\")\n",
    "    network = Net()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)    \n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    test_losses = []\n",
    "    test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]   \n",
    "\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "          train_losses.append(loss.item())\n",
    "          train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "          torch.save(network.state_dict(), model_artifact.path + '/model.pth')\n",
    "          torch.save(optimizer.state_dict(), model_artifact.path + '/optimizer.pth')    \n",
    "   \n",
    "       \n",
    "    print(\"started training\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "      train(epoch)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f024658d-6560-4aca-9608-396c6428b10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"scikit-learn\"],\n",
    ")\n",
    "def evaluate(\n",
    "    model_artifact: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    scalar_metrics: Output[Metrics],\n",
    "    x_test_pickle: Input[Dataset],\n",
    "    y_test_pickle: Input[Dataset],\n",
    "):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    with open(x_test_pickle.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(320, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x)          \n",
    "    \n",
    "\n",
    "    network = model.load_state_dict(torch.load(model_artifact.path + '/model.pth', weights_only=True))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    # Set the model to evaluation mode. This is important as certain layers like dropout behave differently during training and evaluation.\n",
    "    network.eval()\n",
    "\n",
    "    # Lists to store all predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    # Define the class labels for the Fashion MNIST dataset.\n",
    "    classes = ('zero','one','two','three','four','five','six','seven','eight','nine')\n",
    "\n",
    "    # We don't want to compute gradients during evaluation, hence wrap the code inside torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Iterate over all batches in the test loader\n",
    "        for images, labels in test_loader:\n",
    "            # Transfer images and labels to the computational device (either CPU or GPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Pass the images through the model to get predictions\n",
    "            outputs = network(images)\n",
    "\n",
    "            # Get the class with the maximum probability as the predicted class\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Extend the all_preds list with predictions from this batch\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Extend the all_labels list with true labels from this batch\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Print a classification report which provides an overview of the model's performance for each class\n",
    "    print(classification_report(all_labels, all_preds, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46696c0c-6cfc-4ed8-b6eb-59a832aa89b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/9a4857b5-4163-4749-a1ec-28cb91f9422c\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/e14396d4-9c36-4847-8d28-696b9aca5166\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=e14396d4-9c36-4847-8d28-696b9aca5166)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"pytorch_mnist_pipeline\",\n",
    ")\n",
    "def mnist_pipeline(epochs: int):\n",
    "    data = (\n",
    "        load_data()\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_cpu_request(\"2\")\n",
    "    )\n",
    "    preprocess = (\n",
    "        preprocess_data(\n",
    "            x_train_pickle=data.outputs[\"x_train_pickle\"],\n",
    "            y_train_pickle=data.outputs[\"y_train_pickle\"],\n",
    "            x_test_pickle=data.outputs[\"x_test_pickle\"],\n",
    "            y_test_pickle=data.outputs[\"y_test_pickle\"],\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    preprocess.after(data)\n",
    "    model = (\n",
    "        train(\n",
    "            input_size=preprocess.outputs[\"input_size\"],\n",
    "            num_labels=preprocess.outputs[\"num_labels\"],\n",
    "            epochs=epochs,\n",
    "            x_train_pickle=preprocess.outputs[\"x_train_prep\"],\n",
    "            y_train_pickle=preprocess.outputs[\"y_train_prep\"],\n",
    "        )\n",
    "        # .set_memory_limit(\"6G\")\n",
    "        # .set_memory_request(\"6G\")\n",
    "        # .set_cpu_limit(\"1\")\n",
    "        # .set_cpu_request(\"1\")\n",
    "    )\n",
    "    model.after(preprocess)\n",
    "    evaluation = (\n",
    "        evaluate(\n",
    "            model_artifact=model.outputs[\"model_artifact\"],\n",
    "            x_test_pickle=preprocess.outputs[\"x_test_prep\"],\n",
    "            y_test_pickle=preprocess.outputs[\"y_test_prep\"],\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    evaluation.after(model)\n",
    "\n",
    "\n",
    "client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    mnist_pipeline,\n",
    "    arguments={\"epochs\": 2},\n",
    "    experiment_name=\"mnist_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b706c9b-8c1b-4db5-a0ff-6c2d48d528cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574b9ed-e34d-455c-88bf-a22fa0af6373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
